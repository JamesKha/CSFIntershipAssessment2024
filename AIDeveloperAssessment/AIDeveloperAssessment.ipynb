{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assume you have a working ML model that can process individual images and identify carrots, how would you adapt that model such that you could feed it live video inside a grocery store and have it create a record of any carrots it sees.\n",
    "\n",
    "To adapt the model for live video processing in a grocery store, the input pipeline must be reconfigured to handle video input streams instead of static images. This adjustment allows the model to continuously analyze each frame of the video feed to identify carrots. It's essential for the model to differentiate between frames to avoid counting the same carrot multiple times. When a carrot is identified, the model should create a data record containing its location within the frame and any other relevant details, such as size or orientation. This approach ensures that the model can accurately track carrots in real-time video, providing a detailed record of their presence and locations within the grocery store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy example \n",
    "\n",
    "I implemented a toy machine learning concept to demonstrate basic classification techniques. The implementation compares the performance of several classifiers, including Random Forest, Extra Trees, Decision Tree, and LightGBM, on a given dataset. Importantly, this is a surface test of the models' performance without any parameter tuning. The evaluation metrics used are accuracy, precision, and F1-score. This toy implementation provides a basic understanding of how different classifiers perform on classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001003</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001005</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LP001006</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2583</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LP001008</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
       "0  LP001002   Male      No          0      Graduate            No   \n",
       "1  LP001003   Male     Yes          1      Graduate            No   \n",
       "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
       "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
       "4  LP001008   Male      No          0      Graduate            No   \n",
       "\n",
       "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "0             5849                0.0         NaN             360.0   \n",
       "1             4583             1508.0       128.0             360.0   \n",
       "2             3000                0.0        66.0             360.0   \n",
       "3             2583             2358.0       120.0             360.0   \n",
       "4             6000                0.0       141.0             360.0   \n",
       "\n",
       "   Credit_History Property_Area Loan_Status  \n",
       "0             1.0         Urban           Y  \n",
       "1             1.0         Rural           N  \n",
       "2             1.0         Urban           Y  \n",
       "3             1.0         Urban           Y  \n",
       "4             1.0         Urban           Y  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"data.csv\")\n",
    "# Display the first few rows of the DataFrame to inspect the data\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of the DataFrame where 'Loan_Status' is 'Y', sampling 89 rows\n",
    "Loan_Status = df.loc[df['Loan_Status'] == 'Y'].sample(n=89, random_state=42)\n",
    "# Create a subset of the DataFrame where 'Loan_Status' is 'N'\n",
    "noLoan_Status = df.loc[df['Loan_Status'] == 'N']\n",
    "# Concatenate the two subsets to create a new DataFrame\n",
    "df = pd.concat([Loan_Status, noLoan_Status])\n",
    "# Separate the features (X) and the target variable (y)\n",
    "X, y = df.drop('Loan_Status', axis=1), df['Loan_Status']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Credit_History\n",
       "Yes    176\n",
       "No      84\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.replace({'Credit_History': {1.0: 'Yes', 0.0:'No'}})\n",
    "X = X.drop('Loan_ID',axis=1)\n",
    "X.Credit_History.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target variable 'y' to a categorical type\n",
    "y = y.astype('category')\n",
    "# Convert the categorical variable 'y' to numerical codes\n",
    "y = y.cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Married : 2\n",
      "Dependents : 5\n",
      "Education : 2\n",
      "Self_Employed : 3\n",
      "Credit_History : 3\n",
      "Property_Area : 3\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical features (columns) in the DataFrame 'X'\n",
    "cat_feats = X.dtypes[X.dtypes == 'object'].index.tolist()\n",
    "# Identify numerical features (columns) in the DataFrame 'X'\n",
    "num_feats = X.dtypes[~X.dtypes.index.isin(cat_feats)].index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom transformer class that applies a function to a DataFrame\n",
    "class DataframeFunctionTransformer():\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    # Apply the function to the input DataFrame\n",
    "    def transform(self, input_df, **transform_params):\n",
    "        return self.func(input_df)\n",
    "\n",
    "    # Fit method does nothing as the transformation does not require training\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'ApplicantIncome' and 'CoapplicantIncome', take the logarithm, and store the result in a new column\n",
    "def combineAndLog(dataFrame):\n",
    "    dataFrame[\"combinedIncomesLog\"] = dataFrame['ApplicantIncome'] + dataFrame['CoapplicantIncome']\n",
    "    dataFrame[\"combinedIncomesLog\"] = dataFrame[\"combinedIncomesLog\"].apply(lambda x: np.log(x))\n",
    "    return dataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ColumnTransformer] ........... (1 of 2) Processing num, total=   0.0s\n",
      "[ColumnTransformer] ........... (2 of 2) Processing cat, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.24028679, -1.06409999, -1.87641935, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.29071898, -1.06409999, -1.19528473, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.45135659,  0.6837092 ,  1.00097993, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       ...,\n",
       "       [-0.84733623, -1.06409999, -1.37695752, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [-0.26284516, -1.06409999, -0.49596344, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.39697213, -1.06409999, -1.1381979 , ...,  0.        ,\n",
       "         0.        ,  1.        ]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply logarithm transformation to 'LoanAmount' and store the result in a new column 'loanLog'\n",
    "def logTransformer(dataFrame):\n",
    "    dataFrame['loanLog'] = dataFrame[\"LoanAmount\"].apply(lambda x: np.log(x))\n",
    "    return dataFrame\n",
    "# Import necessary library for splitting data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the features (X) and target variable (y) into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=12)\n",
    "# Import necessary libraries for preprocessing data\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Define a pipeline for numerical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values using mean\n",
    "    ('scaler', PowerTransformer())  # Scale the features using PowerTransformer\n",
    "])\n",
    "\n",
    "# Define a pipeline for categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values using most frequent value\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))  # Encode categorical features using OneHotEncoder\n",
    "])\n",
    "\n",
    "# Combine the transformers for numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_feats),  # Apply numeric transformer to numerical features\n",
    "        ('cat', categorical_transformer, cat_feats)  # Apply categorical transformer to categorical features\n",
    "    ],\n",
    "    remainder='drop',  # Drop any remaining columns that are not transformed\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fit and transform the preprocessor on the training data\n",
    "preprocessor.fit_transform(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for machine learning models\n",
    "import lightgbm\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Define names and objects for different classifiers\n",
    "model_names = [\"RandomForestClassifier\", \"ExtraTreesClassifier\", \"DecisionTreeClassifier\", \"LGBMClassifier\"]\n",
    "models = [\n",
    "    RandomForestClassifier(random_state=1),\n",
    "    ExtraTreesClassifier(random_state=1),\n",
    "    DecisionTreeClassifier(random_state=1),\n",
    "    LGBMClassifier(random_state=1)\n",
    "]\n",
    "zipped_combo = zip(model_names, models)\n",
    "\n",
    "# Import necessary metrics for evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, make_scorer\n",
    "\n",
    "# Define a function to fit a classifier pipeline and evaluate its performance\n",
    "def fit_classifier(pipeline, x_train, y_train, x_test, y_test):\n",
    "    model_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = model_fit.predict(x_test)\n",
    "    modelAccuracy_score = accuracy_score(y_test, y_pred)\n",
    "    print('accuracy_score:', accuracy_score(y_test, y_pred))\n",
    "    print('precision:', precision_score(y_test, y_pred))\n",
    "    print('f1_score:', f1_score(y_test, y_pred))\n",
    "    print(\"\\n\")\n",
    "    return modelAccuracy_score\n",
    "\n",
    "# Define a function to test multiple classifiers and return their assessment\n",
    "def classifier(classifier, X_train, y_train, X_test, y_test):\n",
    "    assessment = []\n",
    "    for model_Name, model in classifier:\n",
    "        model_test_pipe = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "        print('Name of model that is being tested: ', model_Name)\n",
    "        modelTest = fit_classifier(model_test_pipe, X_train, y_train, X_test, y_test)\n",
    "        assessment.append((model_Name, modelTest))\n",
    "    return assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of model that is being tested:  RandomForestClassifier\n",
      "[ColumnTransformer] ........... (1 of 2) Processing num, total=   0.0s\n",
      "[ColumnTransformer] ........... (2 of 2) Processing cat, total=   0.0s\n",
      "accuracy_score: 0.6705882352941176\n",
      "precision: 0.5789473684210527\n",
      "f1_score: 0.44\n",
      "\n",
      "\n",
      "Name of model that is being tested:  ExtraTreesClassifier\n",
      "[ColumnTransformer] ........... (1 of 2) Processing num, total=   0.0s\n",
      "[ColumnTransformer] ........... (2 of 2) Processing cat, total=   0.0s\n",
      "accuracy_score: 0.6705882352941176\n",
      "precision: 0.5517241379310345\n",
      "f1_score: 0.5333333333333333\n",
      "\n",
      "\n",
      "Name of model that is being tested:  DecisionTreeClassifier\n",
      "[ColumnTransformer] ........... (1 of 2) Processing num, total=   0.0s\n",
      "[ColumnTransformer] ........... (2 of 2) Processing cat, total=   0.0s\n",
      "accuracy_score: 0.6470588235294118\n",
      "precision: 0.5151515151515151\n",
      "f1_score: 0.53125\n",
      "\n",
      "\n",
      "Name of model that is being tested:  LGBMClassifier\n",
      "[ColumnTransformer] ........... (1 of 2) Processing num, total=   0.0s\n",
      "[ColumnTransformer] ........... (2 of 2) Processing cat, total=   0.0s\n",
      "[LightGBM] [Info] Number of positive: 58, number of negative: 138\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001248 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 191\n",
      "[LightGBM] [Info] Number of data points in the train set: 196, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.295918 -> initscore=-0.866811\n",
      "[LightGBM] [Info] Start training from score -0.866811\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "accuracy_score: 0.6941176470588235\n",
      "precision: 0.6\n",
      "f1_score: 0.5357142857142857\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\james\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\james\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    }
   ],
   "source": [
    "# Apply the classifier function to the zipped combo of model names and models, and the training and testing data\n",
    "result = classifier(zipped_combo, X_train, y_train, X_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
